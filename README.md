# Bia Energy - Case

Autor: Jhonatan Andres Saldarriaga I.
GitHub: [JhonatanS93-DE](https://github.com/JhonatanS93-DE)

---

## Objetivo del Proyecto

Desarrollar una soluciÃ³n de punta a punta para enriquecer datos geoespaciales usando la API de [postcodes.io](https://postcodes.io), integrarlos a una base de datos PostgreSQL, y generar reportes optimizados que permitan anÃ¡lisis rÃ¡pido y calidad de datos.

---

## TecnologÃ­as y herramientas utilizadas
- **Python 3.10**
- **Docker + Docker Compose**
- **PostgreSQL 14**
- **Pandas, Requests, SQLAlchemy**
- **Postcodes.io API**
- **Git y GitHub**

---

## Estructura del Proyecto

```
.
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ postcodes_geo.csv
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ top_postcodes.csv
â”‚   â””â”€â”€ quality_stats.csv
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ bia_pipeline.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â””â”€â”€ bia_pipeline_dag.py
```

---

## EjecuciÃ³n del Proyecto (Local via Docker)

### 1. Clona el repositorio
```bash
git clone https://github.com/JhonatanS93-DE/bia-energy-case.git
cd bia-energy-case
```

### 2. Coloca el archivo `postcodes_geo.csv` en la carpeta `data/`

### 3. Construye y ejecuta los contenedores:
```bash
docker-compose up --build
```

---

## Flujo de datos

1. **Ingesta de CSV**: Se eliminan duplicados, se validan columnas lat/lon.
2. **Enriquecimiento con API**: Se consulta `postcodes.io` para obtener el cÃ³digo postal y paÃ­s correspondiente. 
   - Control de errores por timeouts, fallas HTTP y respuestas vacÃ­as.
   - Se respeta el rate limit con `sleep(1)`.
3. **Carga en PostgreSQL**: Se crea la tabla `enriched_postcodes`.
4. **GeneraciÃ³n de reportes**:
   - `top_postcodes.csv`: los cÃ³digos postales mÃ¡s comunes.
   - `quality_stats.csv`: % de coordenadas no enriquecidas.

---

## Porque esta solucion?
- Se eligiÃ³ **PostgreSQL** por su robustez y rendimiento para operaciones analÃ­ticas.
- El proyecto estÃ¡ contenedorizado para asegurar portabilidad y reproducibilidad.
- La arquitectura puede escalarse en un entorno de Airflow o pipeline cloud en AWS.
- Se aplicaron principios de observabilidad mediante logs estructurados.

---

## Mejoras para hacerlo una solucion escalable

### Escalabilidad con Airflow (implementaciÃ³n adicional)
Como mejora pensada para entornos productivos y escalables, se propone integrar Apache Airflow para orquestar el pipeline. Esto permitirÃ¡:

- Ejecutar tareas de ingestiÃ³n, enriquecimiento, almacenamiento y reporte de forma secuencial y automatizada.
- Monitorizar y programar la ejecuciÃ³n diaria/semanal de procesamiento de datos.
- Manejar reintentos automÃ¡ticos ante errores.

Esta versiÃ³n con Airflow se entrega como una carpeta adicional (`airflow/`) que contiene un DAG de ejemplo para ejecutar el flujo completo, permitiendo demostrar cÃ³mo se adapta la soluciÃ³n a escenarios reales de orquestaciÃ³n de datos.

- Incorporar cache local para evitar peticiones duplicadas a la API.
- Test unitarios con `pytest`.

---

## Diagrama de Arquitectura

El siguiente diagrama muestra el flujo de datos completo del pipeline:

ðŸ“„ [Ver diagrama editable en draw.io](docs/bia_pipeline_detailed.drawio)

Puedes abrirlo desde [https://app.diagrams.net](https://app.diagrams.net) arrastrando el archivo aqui se podria editar una mejora futura.

---

## Explicacion final

Este proyecto refleja prÃ¡cticas de ingenierÃ­a de datos modernas: pipelines modulares, manejo  de APIs, almacenamiento eficiente y entrega de reportes. Se desarrollÃ³ con enfoque en calidad, mantenibilidad y ejecuciÃ³n realista bajo condiciones de producciÃ³n controlada.

---

Para dudas o retroalimentaciÃ³n: **jhonatan1393@gmail.com**
